---
kind: secret
name: {{ include "clickhouse.secretServerName" . }}
description: Clickhouse server startup script
tags: {{- include "clickhouse.tags" . | nindent 4 }}
type: opaque
data:
  encoding: plain
  payload: |-
    #!/usr/bin/env bash
    set -euo pipefail

    # --- Runtime variables ---
    LOCATION=$(basename "${CPLN_LOCATION:-}")
    HOSTNAME=${HOSTNAME:-}
    GVC="{{ .Values.gvc.name }}"
    WORKLOAD_NAME="{{ include "clickhouse.serverName" . }}"
    KEEPER_WORKLOAD_NAME="{{ include "clickhouse.keeperName" . }}"
    REPLICA_INDEX=$(echo "${HOSTNAME}" | awk -F'-' '{print $NF}')
    SELF_FQDN="replica-${REPLICA_INDEX}.${WORKLOAD_NAME}.${LOCATION}.${GVC}.cpln.local"

    echo "Starting ClickHouse server for replica ${SELF_FQDN}"

    # --- Directories ---
    CONFIG_DIR="/etc/clickhouse-server"
    LOG_DIR="/var/log/clickhouse-server"
    DATA_DIR="/var/lib/clickhouse"

    mkdir -p "$CONFIG_DIR" "$LOG_DIR" "$DATA_DIR"

    # --- Parse locations and replicas ---
    IFS=',' read -r -a LOCATIONS <<< "${LOCATIONS_STR:-}"

    # --- Compute SHARD_INDEX for this node (per location, not per replica) ---
    SHARD_INDEX=1
    for LOC_PAIR in "${LOCATIONS[@]}"; do
        LOC="${LOC_PAIR%%:*}"
        if [[ "$LOC" == "$LOCATION" ]]; then
            break
        fi
        SHARD_INDEX=$((SHARD_INDEX+1))
    done

    # --- Generate config.xml ---
    cat > "$CONFIG_DIR/config.xml" <<EOF
    <clickhouse>

        <!-- Users config -->
        <users_config>/etc/clickhouse-server/users.xml</users_config>

        <logger>
            <level>information</level>
        </logger>

        <!--
        Isolate temporary files from main data.
        Strongly recommended for containerized and object-storage-backed deployments.
        -->
        <tmp_path>/var/lib/clickhouse-tmp</tmp_path>

        <!--
        Prevent large merge operations from causing disk or memory spikes.
        These conservative defaults favor stability over raw throughput.
        -->
        <max_bytes_to_merge_at_max_space_in_pool>1073741824</max_bytes_to_merge_at_max_space_in_pool> <!-- 1 GiB -->
        <max_bytes_to_merge_at_min_space_in_pool>536870912</max_bytes_to_merge_at_min_space_in_pool>  <!-- 512 MiB -->

        <!--
        Cap total temporary data on disk to prevent runaway spill.
        Set to 0 (unlimited) only for controlled environments.
        -->
        <max_temporary_data_on_disk_size>21474836480</max_temporary_data_on_disk_size> <!-- 20 GiB -->
        
        <!-- Ports -->
        <tcp_port>9000</tcp_port>
        <http_port>8123</http_port>
        <interserver_http_port>9009</interserver_http_port>
        <interserver_http_host>$SELF_FQDN</interserver_http_host>

        <!-- Listen on all interfaces -->
        <listen_host>0.0.0.0</listen_host>
        <listen_host>::</listen_host>

        <!-- Display name -->
        <display_name>$WORKLOAD_NAME node $REPLICA_INDEX</display_name>
        <distributed_ddl>
            <path>/clickhouse/task_queue/ddl</path>
            <profile>default</profile>
        </distributed_ddl>
        <!-- Remote servers / cluster -->
        <remote_servers>
            <{{ .Values.clusterName }}>
    EOF

    # --- Add shards and replicas ---
    for LOC_PAIR in "${LOCATIONS[@]}"; do
        LOC="${LOC_PAIR%%:*}"
        REPLICAS="${LOC_PAIR##*:}"
        {
            echo "            <shard>"
            echo "                <internal_replication>true</internal_replication>"
            for IDX in $(seq 0 $((REPLICAS-1))); do
                FQDN="replica-$IDX.${WORKLOAD_NAME}.${LOC}.${GVC}.cpln.local"
                echo "                <replica>"
                echo "                    <host>$FQDN</host>"
                echo "                    <port>9000</port>"
                echo "                    <user>default</user>"
                echo "                    <password>${CLICKHOUSE_PASSWORD}</password>"
                echo "                    <default_database>${CLICKHOUSE_DATABASE}</default_database>"
                echo "                </replica>"
            done
            echo "            </shard>"
        } >> "$CONFIG_DIR/config.xml"
    done

    # --- Close remote_servers ---
    {
        echo "        </{{ .Values.clusterName }}>"
        echo "    </remote_servers>"
    } >> "$CONFIG_DIR/config.xml"

    # --- Zookeeper nodes ---
    {
    echo "  <zookeeper>"
    for LOC in "${LOCATIONS[@]:0:3}"; do       # only first 3 locations
        LOC_NAME="${LOC%%:*}"
        FQDN="replica-0.${KEEPER_WORKLOAD_NAME}.${LOC_NAME}.${GVC}.cpln.local"
        echo "    <node>"
        echo "      <host>$FQDN</host>"
        echo "      <port>9181</port>"
        echo "    </node>"
    done
    echo "  </zookeeper>"
    } >> "$CONFIG_DIR/config.xml"

    # --- Macros ---
    {
        echo "    <macros>"
        printf "        <shard>%d</shard>\n" "$SHARD_INDEX"
        printf "        <replica>%d</replica>\n" "$((REPLICA_INDEX + 1))"
        echo "    </macros>"
    } >> "$CONFIG_DIR/config.xml"

    # --- Add system logging for filesystem cache and query visibility ---
    cat >> "$CONFIG_DIR/config.xml" <<EOF

        <!-- System logs settings -->
        <system_logs>
            <filesystem_cache_log>
                <database>system</database>
                <table>filesystem_cache_log</table>
                <engine>Log</engine>
                <flush_interval_milliseconds>7500</flush_interval_milliseconds>
            </filesystem_cache_log>
        </system_logs>
    EOF

    # --- Close XML ---
    echo "</clickhouse>" >> "$CONFIG_DIR/config.xml"

    echo "Generated ClickHouse config at $CONFIG_DIR/config.xml"

    # --- Generate users.xml dynamically ---
    cat > "$CONFIG_DIR/users.xml" <<EOF
    <clickhouse>
        <users>
            <default>
                <password>${CLICKHOUSE_PASSWORD}</password>
                <profile>default</profile>
                <quota>default</quota>
            </default>
        </users>

        <profiles>
            <default>
                <max_memory_usage>10000000000</max_memory_usage>
                <use_uncompressed_cache>0</use_uncompressed_cache>
                <load_balancing>in_order</load_balancing>
                <log_queries>1</log_queries>
            </default>
        </profiles>
    </clickhouse>
    EOF

    echo "Generated ClickHouse users.xml at $CONFIG_DIR/users.xml"

{{- if .Values.aws.enabled }}
    mkdir -p /var/lib/clickhouse/disks/s3_disk_metadata
    mkdir -p /var/lib/clickhouse_cache/s3
    echo "1" > /var/lib/clickhouse/disks/s3_disk_metadata/format_version.txt
    echo "Created paths for S3"
{{- end }}

    # --- Force ClickHouse to resolve local FQDN as localhost ---
    LOCAL_FQDN="$SELF_FQDN"

    echo "Applying hostname mapping workaround..."
    echo "127.0.0.1   $LOCAL_FQDN" >> /etc/hosts

    echo "Hosts file updated:"
    cat /etc/hosts

    # --- Start ClickHouse server in the background ---
    clickhouse-server --config-file="$CONFIG_DIR/config.xml" &

    # Wait for server to accept connections using default superuser
    echo "Waiting for ClickHouse to accept connections..."
    until clickhouse-client --query "SELECT 1" &>/dev/null; do
        sleep 1
    done

    echo "ClickHouse is ready!"

    # --- Compute if this node is the bootstrap node for cluster-wide DB creation ---
    FIRST_LOC="{{ (index .Values.gvc.locations 0).name }}"
    FIRST_LOCATION="false"

    if [[ "$LOCATION" == "$FIRST_LOC" && "$REPLICA_INDEX" == "0" ]]; then
        FIRST_LOCATION="true"
    fi

    # --- Wait for Keeper quorum before creating database ---
    if [[ "$FIRST_LOCATION" == "true" ]]; then
        echo "Waiting for Keeper quorum before bootstrapping DB..."
        until clickhouse-client --query "SELECT * FROM system.zookeeper WHERE path='/'" >/dev/null 2>&1; do
            echo "Keeper not ready yet, waiting..."
            sleep 2
        done
        echo "Keeper quorum detected!"
    fi

    # --- Create database using default user (only on first location / first replica) ---
    if [[ "$FIRST_LOCATION" == "true" ]]; then
        echo "Bootstrapping ClickHouse database across cluster (ON CLUSTER)..."
        clickhouse-client --user default --password "$CLICKHOUSE_PASSWORD" \
            --query "CREATE DATABASE IF NOT EXISTS ${CLICKHOUSE_DATABASE} ON CLUSTER {{ .Values.clusterName }} ENGINE = Atomic;"
    else
        echo "Skipping DB creation on this replica (not the bootstrap node)."
    fi

    # Bring ClickHouse server to foreground
    wait